{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d4f956",
   "metadata": {},
   "source": [
    "# NEXT word prediction\n",
    "# Using Reuters data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5710ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Libraries\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2db370d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sentence limit: otherwise leads to memory error in devices with less memory\n",
    "sentCntLimit = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24db4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word mapping\n",
    "sentCnt = 0\n",
    "uniq_words = set()\n",
    "for sentence in reuters.sents():\n",
    "    if sentCnt<sentCntLimit:\n",
    "        sentCnt += 1\n",
    "    else:\n",
    "        break\n",
    "    # clean the text: lower case, only alphabet\n",
    "    sentence = [word.lower() for word in sentence]\n",
    "    sentence = [re.sub(\"[^a-zA-Z]\", '', word) for word in sentence]\n",
    "    sentence = [re.sub(r\"'s/b\", '', word) for word in sentence]\n",
    "    # remove empty string\n",
    "    for word in sentence:\n",
    "        if word == '':\n",
    "            sentence.remove(word)\n",
    "    # replace 'u' followed by 's' by 'us'\n",
    "    i=0\n",
    "    while (1):\n",
    "        if (i<len(sentence)-1):\n",
    "          if (sentence[i]=='u') & (sentence[i+1]=='s'):\n",
    "            sentence.pop(i)\n",
    "            sentence.pop(i)\n",
    "            sentence.insert(i, 'usa')\n",
    "        if (i>=len(sentence)-1):\n",
    "            break\n",
    "        else:\n",
    "            i+=1\n",
    "    # now go thru the words in sentence and add the new ones to uniq_words set\n",
    "    for word in sentence:\n",
    "        uniq_words.add(word)\n",
    "        #print(uniq_words)\n",
    "        #x1=input(\"Enter something: \")\n",
    "uniq_words = sorted(uniq_words)\n",
    "mapping = dict((word, ind) for ind, word in enumerate(uniq_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61e56c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41018, 155)\n"
     ]
    }
   ],
   "source": [
    "# create sequences\n",
    "sentCnt = 0\n",
    "all_seq = []\n",
    "for sentence in reuters.sents():\n",
    "    if sentCnt<sentCntLimit:\n",
    "        sentCnt += 1\n",
    "    else:\n",
    "        break\n",
    "    sentence = [word.lower() for word in sentence]\n",
    "    sentence = [re.sub(\"[^a-zA-Z]\", '', word) for word in sentence]\n",
    "    sentence = [re.sub(r\"'s/b\", '', word) for word in sentence]\n",
    "    # remove empty string\n",
    "    for word in sentence:\n",
    "        if word == '':\n",
    "            sentence.remove(word)\n",
    "    # replace 'u' followed by 's' by 'us'\n",
    "    i=0\n",
    "    while (1):\n",
    "        if (i<len(sentence)-1):\n",
    "          if (sentence[i]=='u') & (sentence[i+1]=='s'):\n",
    "            sentence.pop(i)\n",
    "            sentence.pop(i)\n",
    "            sentence.insert(i, 'usa')\n",
    "        if (i>=len(sentence)-1):\n",
    "            break\n",
    "        else:\n",
    "            i+=1\n",
    "\n",
    "    # create sequences of 2 words to all words and append them to all_seq\n",
    "    for i in range(2, len(sentence)):\n",
    "        seq1 = []\n",
    "        for w1 in sentence[:i]:\n",
    "            seq1.append(mapping[w1])\n",
    "        all_seq.append(seq1)\n",
    "\n",
    "# find the max_len among all sequences in all_seq\n",
    "max_len = max(len(all_seq[i]) for i in range(len(all_seq)))\n",
    "\n",
    "# pre-pad all sequences for length max_len\n",
    "all_seq = pad_sequences(all_seq, maxlen=max_len, truncating='pre')\n",
    "\n",
    "# create X and y from all_seq\n",
    "all_seq = np.array(all_seq)\n",
    "X = all_seq[:, :-1]\n",
    "y = all_seq[:, -1]\n",
    "\n",
    "# use to_categorical to convert each y value to a vector of length of uniq_words\n",
    "y = to_categorical(y, len(uniq_words))\n",
    "\n",
    "# split X, y into training and validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state = 42)\n",
    "print(X_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fb5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1145/1282 [=========================>....] - ETA: 1:20 - loss: 6.7445 - acc: 0.0633"
     ]
    }
   ],
   "source": [
    "# create a LSTM model: 1st layer is Embedding, 2nd is LSTM, 3rd is a Dense layer with softmax\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(uniq_words), 50, input_length = max_len-1, trainable=True))\n",
    "model.add(LSTM(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(len(uniq_words), 'softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics='acc', optimizer='adam')\n",
    "# use higher epochs while actually testing\n",
    "model.fit(X_tr, y_tr, epochs=2, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ce248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict text\n",
    "seed_text = \"asian exporters\"\n",
    "seed_words = seed_text.split()\n",
    "cntPred = 5\n",
    "for i in range(cntPred):\n",
    "    # map seed_text using mapping\n",
    "    map_words = [mapping[w1] for w1 in seed_words]\n",
    "    # use pad_sequences to pad 0s to map_words\n",
    "    pad_input = pad_sequences([map_words], maxlen=max_len-1, truncating='pre')\n",
    "    # predict\n",
    "    yhat_probs = model.predict(pad_input, verbose=1)\n",
    "    # get the index of the max prob among yhat_probs\n",
    "    yhat = np.argmax(yhat_probs)\n",
    "    for word, ind1 in mapping.items():\n",
    "        if yhat==ind1:\n",
    "            seed_words.append(word)\n",
    "            break\n",
    "print(seed_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
